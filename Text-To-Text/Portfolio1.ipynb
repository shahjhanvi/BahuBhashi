{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BahuBhashi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is BahuBhashi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently many MNC’s are facing an issue where a customer of a particular region calls in and the customer experiences long wait hours as all the representatives are busy. While their call centers in other regions are not experiencing busy wait hours. To mitigate this issue there could be a system designed which takes an audio input in one language and translates it into another language. This solution could be used where if the call centers in one region are facing high call volumes the calls could be directed to the regions facing lower call volumes and BahuBhashi could be used so that person on both ends can converse in their own language and find a solution to their issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples of translation: English-To-German "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Results.JPG\" alt=\"Drawing\" style=\"width: 1000px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the requirements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tOperating System: Windows 10/ macOS High Sierra\n",
    "\n",
    "•\tDataset: European Parliament Proceedings Parallel Corpus (http://www.statmt.org/europarl/v7/de-en.tgz)\n",
    "\n",
    "•\tPython: v3.6.2\n",
    "\n",
    "•\tTensorflow: v1.8\n",
    "\n",
    "•\tNatural Language ToolKit (Nltk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does translation takes place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regular (non-recurrent) neural network is a generic machine learning algorithm that takes in a list of numbers and calculates a result (based on previous training). Neural networks can be used as a black box to solve lots of problems.\n",
    "\n",
    "A recurrent neural network (or RNN for short) is a slightly tweaked version of a neural network where the previous state of the neural network is one of the inputs to the next calculation. This means that previous calculations change the results of future calculations!\n",
    "\n",
    "This trick allows neural networks to learn patterns in a sequence of data. For example, you can use it to predict the next most likely word in a sentence based on the first few words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RNN1.gif\" alt=\"Drawing\" style=\"width: 1000px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the RNN has a “memory” of each word that passed through it, the final encoding that it calculates represents all the words in the sentence.\n",
    "\n",
    "To generate this encoding, the sentence is fed into the RNN, one word at time. The final result after the last word is processed and will be the values that represent the entire sentence.\n",
    "\n",
    "This is a way to represent an entire sentence as a set of unique numbers. It is not known what each number in the encoding means, but it doesn’t really matter. As long as each sentence is uniquely identified by it’s own set of numbers, there is no need to know exactly how those numbers were generated. Now it is known how to use an RNN to encode a sentence into a set of unique numbers. How does that help? What if two RNNs were taken and hooked end-to-end? The first RNN could generate the encoding that represents a sentence. Then the second RNN could take that encoding and just do the same logic in reverse to decode the original sentence again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RNN2.png\" alt=\"Drawing\" style=\"width: 1000px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to encode and then decode the original sentence again isn’t very useful.The second RNN is trained to decode the sentence into German instead of English by using parallel corpora training data to train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RNN3.png\" alt=\"Drawing\" style=\"width: 1000px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries and dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\pares\\anaconda3\\lib\\site-packages (3.3)\n",
      "Requirement already satisfied: six in c:\\users\\pares\\anaconda3\\lib\\site-packages (from nltk) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import NMT_Model\n",
    "import nmt_data_utils\n",
    "import nmt_model_utils\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the datasets and displaying the datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the english texts\n",
    "with open('europarl-v7.de-en.en','r', encoding = 'utf-8') as f:\n",
    "        en = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the german texts\n",
    "with open('europarl-v7.de-en.de','r',encoding = 'utf-8') as f:\n",
    "    de = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en), len(de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 5 sentence pairs. \n",
    "for line in zip(en[:5], de[:5]):\n",
    "    print(line, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary new lines. \n",
    "de = [line.strip() for line in de]\n",
    "en = [line.strip() for line in en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will only use sentences of similar lengths in order to make training easier. \n",
    "len_en = [len(sent) for sent in en if 20 < len(sent) < 50]\n",
    "len_dist = Counter(len_en).most_common()\n",
    "len_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 158238 sentences that contain betwenn 20 and 50 words.\n",
    "len(len_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_de = []\n",
    "_en = []\n",
    "for sent_de, sent_en in zip(de, en):\n",
    "    if 20 < len(sent_en) < 50:\n",
    "        _de.append(sent_de)\n",
    "        _en.append(sent_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but we will not use all 150 000 sentences, only 5000 for the beginning. \n",
    "en_preprocessed, en_most_common = nmt_data_utils.preprocess(_en[:5000])\n",
    "de_preprocessed, de_most_common = nmt_data_utils.preprocess(_de[:5000], language = 'german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_preprocessed), len(de_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further cleaning of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some of the sentences there is not german or english counterpart, i.e. only an empy array []\n",
    "# therefore we will remove those sentence pairs.\n",
    "en_preprocessed_clean, de_preprocessed_clean = [], []\n",
    "\n",
    "for sent_en, sent_de in zip(en_preprocessed, de_preprocessed):\n",
    "    if sent_en != [] and sent_de != []:\n",
    "        en_preprocessed_clean.append(sent_en)\n",
    "        de_preprocessed_clean.append(sent_de)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_preprocessed_clean), len(de_preprocessed_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the clean sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, d in zip(en_preprocessed_clean, de_preprocessed_clean[:5]):\n",
    "    print('English:\\n', e)\n",
    "    print('German:\\n', d, '\\n'*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_most_common[:15], len(en_most_common), len(de_most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can create our lookup dicts for english and german, i.e. our vocab. \n",
    "# we will also include special tokens, later on used in the model. \n",
    "specials = [\"<unk>\", \"<s>\", \"</s>\", '<pad>']\n",
    "\n",
    "en_word2ind, en_ind2word, en_vocab_size = nmt_data_utils.create_vocab(en_most_common, specials)\n",
    "de_word2ind, de_ind2word, de_vocab_size = nmt_data_utils.create_vocab(de_most_common, specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the size \n",
    "en_vocab_size, de_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to feed the sentences to the network, we have to convert them to ints, corresponding to their indices\n",
    "# in the lookup dicts. \n",
    "# we reverse the source language sentences, i.e. the english sentences as this alleviates learning for the seq2seq \n",
    "# model. Apart from this we also include EndOfSentence and StartOfSentence tags, which are needed as well. \n",
    "en_inds, en_unknowns = nmt_data_utils.convert_to_inds(en_preprocessed_clean, en_word2ind, reverse = True, eos = True)\n",
    "de_inds, de_unknowns = nmt_data_utils.convert_to_inds(de_preprocessed_clean, de_word2ind, sos = True, eos = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[nmt_data_utils.convert_to_words(sentence, en_ind2word) for sentence in  en_inds[:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning the Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams. \n",
    "# those are probably not perfect, but work fine for now. \n",
    "num_layers_encoder = 4\n",
    "num_layers_decoder = 4\n",
    "rnn_size_encoder = 128\n",
    "rnn_size_decoder = 128\n",
    "embedding_dim = 300\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 5 \n",
    "clip = 5\n",
    "keep_probability = 0.8\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay_steps = 1000\n",
    "learning_rate_decay = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the graph and train the model. \n",
    "nmt_model_utils.reset_graph()\n",
    "\n",
    "nmt = NMT_Model.NMT(en_word2ind,\n",
    "                    en_ind2word,\n",
    "                    de_word2ind,\n",
    "                    de_ind2word,\n",
    "                    './models/local_one/my_model',\n",
    "                    'TRAIN',\n",
    "                    embedding_dim = embedding_dim,\n",
    "                    num_layers_encoder = num_layers_encoder,\n",
    "                    num_layers_decoder = num_layers_decoder,\n",
    "                    batch_size = batch_size,\n",
    "                    clip = clip,\n",
    "                    keep_probability = keep_probability,\n",
    "                    learning_rate = learning_rate,\n",
    "                    epochs = epochs,\n",
    "                    rnn_size_encoder = rnn_size_encoder,\n",
    "                    rnn_size_decoder = rnn_size_decoder, \n",
    "                    learning_rate_decay_steps = learning_rate_decay_steps,\n",
    "                    learning_rate_decay = learning_rate_decay)\n",
    "  \n",
    "nmt.build_graph()\n",
    "nmt.train(en_inds, de_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_de_inds, _de_unknowns = nmt_data_utils.convert_to_inds(de_preprocessed_clean, de_word2ind, sos = True,  eos = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the inference model does not necessaryly need to get input batches. we can just give it. the whole input\n",
    "# data, but the the batchsize has to be specified as the lenght of the input data.\n",
    "nmt_model_utils.reset_graph()\n",
    "\n",
    "nmt = NMT_Model.NMT(en_word2ind,\n",
    "                    en_ind2word,\n",
    "                    de_word2ind,\n",
    "                    de_ind2word,\n",
    "                    './models/local_one/my_model',\n",
    "                    'INFER',\n",
    "                    num_layers_encoder = num_layers_encoder,\n",
    "                    num_layers_decoder = num_layers_decoder,\n",
    "                    batch_size = len(en_inds[:50]),\n",
    "                    keep_probability = 1.0,\n",
    "                    learning_rate = 0.0,\n",
    "                    beam_width = 0,\n",
    "                    rnn_size_encoder = rnn_size_encoder,\n",
    "                    rnn_size_decoder = rnn_size_decoder)\n",
    "\n",
    "nmt.build_graph()\n",
    "preds = nmt.infer(en_inds[:50], restore_path =  './models/local_one/my_model', targets = _de_inds[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some of the created translations\n",
    "# Note: the way bleu score is probably not the perfect way to do it\n",
    "#The Bilingual Evaluation Understudy Score, or BLEU for short, is a metric for evaluating a generated sentence to a \n",
    "#reference sentence.\n",
    "\n",
    "A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.\n",
    "nmt_model_utils.sample_results(preds, en_ind2word, de_ind2word, en_word2ind, de_word2ind, _de_inds[:50], en_inds[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Results.JPG\" alt=\"Drawing\" style=\"width: 1000px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://github.com/thomasschmied/Neural_Machine_Translation_Tensorflow\n",
    "2. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.472.1019&rep=rep1&type=pdf\n",
    "3. https://pdfs.semanticscholar.org/0fa1/911622a6c0a3dd43fefbdf2695ebdb7e10fa.pdf\n",
    "4. https://www.cs.cmu.edu/~awb/papers/eurospeech2003/speechalator.pdf\n",
    "5. https://github.com/tensorflow/nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
