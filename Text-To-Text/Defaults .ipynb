{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import NMT_Model\n",
    "import nmt_data_utils\n",
    "import nmt_model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the english texts\n",
    "with open('europarl-v7.de-en.en','r', encoding = 'utf-8') as f:\n",
    "        en = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the german texts\n",
    "with open('europarl-v7.de-en.de','r',encoding = 'utf-8') as f:\n",
    "    de = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1920209, 1920209)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en), len(de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Resumption of the session\\n', 'Wiederaufnahme der Sitzungsperiode\\n') \n",
      "\n",
      "('I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\n', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.\\n') \n",
      "\n",
      "(\"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\n\", 'Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.\\n') \n",
      "\n",
      "('You have requested a debate on this subject in the course of the next few days, during this part-session.\\n', 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen.\\n') \n",
      "\n",
      "(\"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\n\", 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen -, allen Opfern der Stürme, insbesondere in den verschiedenen Ländern der Europäischen Union, in einer Schweigeminute zu gedenken.\\n') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first 5 sentence pairs. \n",
    "for line in zip(en[:5], de[:5]):\n",
    "    print(line, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary new lines. \n",
    "de = [line.strip() for line in de]\n",
    "en = [line.strip() for line in en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(47, 7266),\n",
       " (49, 7113),\n",
       " (45, 6928),\n",
       " (35, 6833),\n",
       " (48, 6813),\n",
       " (21, 6642),\n",
       " (44, 6519),\n",
       " (46, 6491),\n",
       " (43, 6443),\n",
       " (40, 6130),\n",
       " (42, 6108),\n",
       " (37, 5824),\n",
       " (41, 5793),\n",
       " (34, 5711),\n",
       " (39, 5682),\n",
       " (29, 5659),\n",
       " (38, 5599),\n",
       " (33, 5496),\n",
       " (36, 5452),\n",
       " (31, 4651),\n",
       " (32, 4554),\n",
       " (30, 4441),\n",
       " (27, 4117),\n",
       " (28, 4062),\n",
       " (26, 3989),\n",
       " (25, 3911),\n",
       " (24, 3762),\n",
       " (23, 3473),\n",
       " (22, 2776)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will only use sentences of similar lengths in order to make training easier. \n",
    "len_en = [len(sent) for sent in en if 20 < len(sent) < 50]\n",
    "len_dist = Counter(len_en).most_common()\n",
    "len_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158238"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 158238 sentences that contain betwenn 20 and 50 words.\n",
    "len(len_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_de = []\n",
    "_en = []\n",
    "for sent_de, sent_en in zip(de, en):\n",
    "    if 20 < len(sent_en) < 50:\n",
    "        _de.append(sent_de)\n",
    "        _en.append(sent_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pares\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# but we will not use all 150 000 sentences, only 5000 for the beginning. \n",
    "en_preprocessed, en_most_common = nmt_data_utils.preprocess(_en[:5000])\n",
    "de_preprocessed, de_most_common = nmt_data_utils.preprocess(_de[:5000], language = 'german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_preprocessed), len(de_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some of the sentences there is not german or english counterpart, i.e. only an empy array []\n",
    "# therefore we will remove those sentence pairs.\n",
    "en_preprocessed_clean, de_preprocessed_clean = [], []\n",
    "\n",
    "for sent_en, sent_de in zip(en_preprocessed, de_preprocessed):\n",
    "    if sent_en != [] and sent_de != []:\n",
    "        en_preprocessed_clean.append(sent_en)\n",
    "        de_preprocessed_clean.append(sent_de)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4988, 4988)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_preprocessed_clean), len(de_preprocessed_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:\n",
      " ['resumption', 'of', 'the', 'session']\n",
      "German:\n",
      " ['wiederaufnahme', 'der', 'sitzungsperiode'] \n",
      "\n",
      "\n",
      "\n",
      "English:\n",
      " ['please', 'rise', ',', 'then', ',', 'for', 'this', 'minute', \"'\", 's', 'silence', '.']\n",
      "German:\n",
      " ['ich', 'bitte', 'sie', ',', 'sich', 'zu', 'einer', 'schweigeminute', 'zu', 'erheben', '.'] \n",
      "\n",
      "\n",
      "\n",
      "English:\n",
      " ['(', 'the', 'house', 'rose', 'and', 'observed', 'a', 'minute', \"'\", 's', 'silence', ')']\n",
      "German:\n",
      " ['(', 'das', 'parlament', 'erhebt', 'sich', 'zu', 'einer', 'schweigeminute', '.', ')'] \n",
      "\n",
      "\n",
      "\n",
      "English:\n",
      " ['madam', 'president', ',', 'on', 'a', 'point', 'of', 'order', '.']\n",
      "German:\n",
      " ['frau', 'präsidentin', ',', 'zur', 'geschäftsordnung', '.'] \n",
      "\n",
      "\n",
      "\n",
      "English:\n",
      " ['madam', 'president', ',', 'on', 'a', 'point', 'of', 'order', '.']\n",
      "German:\n",
      " ['frau', 'präsidentin', ',', 'zur', 'geschäftsordnung', '.'] \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e, d in zip(en_preprocessed_clean, de_preprocessed_clean[:5]):\n",
    "    print('English:\\n', e)\n",
    "    print('German:\\n', d, '\\n'*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('.', 3981),\n",
       "  ('the', 1864),\n",
       "  ('is', 1371),\n",
       "  ('this', 860),\n",
       "  (',', 842),\n",
       "  ('to', 822),\n",
       "  ('we', 736),\n",
       "  ('i', 677),\n",
       "  ('that', 619),\n",
       "  ('a', 611),\n",
       "  ('of', 592),\n",
       "  ('it', 486),\n",
       "  ('not', 474),\n",
       "  (')', 451),\n",
       "  ('(', 450)],\n",
       " 4135,\n",
       " 5410)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_most_common[:15], len(en_most_common), len(de_most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can create oyr lookup dicts for english and german, i.e. our vocab. \n",
    "# we will also include special tokens, later on used in the model. \n",
    "specials = [\"<unk>\", \"<s>\", \"</s>\", '<pad>']\n",
    "\n",
    "en_word2ind, en_ind2word, en_vocab_size = nmt_data_utils.create_vocab(en_most_common, specials)\n",
    "de_word2ind, de_ind2word, de_vocab_size = nmt_data_utils.create_vocab(de_most_common, specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4139, 5414)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab_size, de_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to feed the sentences to the network, we have to convert them to ints, corresponding to their indices\n",
    "# in the lookup dicts. \n",
    "# we reverse the source language sentences, i.e. the english sentences as this alleviates learning for the seq2seq \n",
    "# model. Apart from this we also include EndOfSentence and StartOfSentence tags, which are needed as well. \n",
    "en_inds, en_unknowns = nmt_data_utils.convert_to_inds(en_preprocessed_clean, en_word2ind, reverse = True, eos = True)\n",
    "de_inds, de_unknowns = nmt_data_utils.convert_to_inds(de_preprocessed_clean, de_word2ind, sos = True, eos = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['session', 'the', 'of', 'resumption', '</s>'],\n",
       " ['.',\n",
       "  'silence',\n",
       "  's',\n",
       "  \"'\",\n",
       "  'minute',\n",
       "  'this',\n",
       "  'for',\n",
       "  ',',\n",
       "  'then',\n",
       "  ',',\n",
       "  'rise',\n",
       "  'please',\n",
       "  '</s>']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[nmt_data_utils.convert_to_words(sentence, en_ind2word) for sentence in  en_inds[:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams. \n",
    "# those are probably not perfect, but work fine for now. \n",
    "num_layers_encoder = 4\n",
    "num_layers_decoder = 4\n",
    "rnn_size_encoder = 128\n",
    "rnn_size_decoder = 128\n",
    "embedding_dim = 300\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 5 \n",
    "clip = 5\n",
    "keep_probability = 0.8\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay_steps = 1000\n",
    "learning_rate_decay = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built.\n",
      "-------------------- Epoch 0 of 5 --------------------\n",
      "Iteration: 0 of 77\ttrain_loss: 8.5966\n",
      "Iteration: 2 of 77\ttrain_loss: 15.3424\n",
      "Iteration: 4 of 77\ttrain_loss: 8.3517\n",
      "Iteration: 6 of 77\ttrain_loss: 7.8113\n",
      "Iteration: 8 of 77\ttrain_loss: 7.1683\n",
      "Iteration: 10 of 77\ttrain_loss: 6.5226\n",
      "Iteration: 12 of 77\ttrain_loss: 6.3681\n",
      "Iteration: 14 of 77\ttrain_loss: 5.9449\n",
      "Iteration: 16 of 77\ttrain_loss: 5.9254\n",
      "Iteration: 18 of 77\ttrain_loss: 5.7976\n",
      "Iteration: 20 of 77\ttrain_loss: 5.3574\n",
      "Iteration: 22 of 77\ttrain_loss: 5.4124\n",
      "Iteration: 24 of 77\ttrain_loss: 5.3263\n",
      "Iteration: 26 of 77\ttrain_loss: 5.3544\n",
      "Iteration: 28 of 77\ttrain_loss: 5.3601\n",
      "Iteration: 30 of 77\ttrain_loss: 5.1930\n",
      "Iteration: 32 of 77\ttrain_loss: 5.1226\n",
      "Iteration: 34 of 77\ttrain_loss: 4.8164\n",
      "Iteration: 36 of 77\ttrain_loss: 4.9414\n",
      "Iteration: 38 of 77\ttrain_loss: 4.5892\n",
      "Iteration: 40 of 77\ttrain_loss: 4.8798\n",
      "Iteration: 42 of 77\ttrain_loss: 4.7126\n",
      "Iteration: 44 of 77\ttrain_loss: 4.5648\n",
      "Iteration: 46 of 77\ttrain_loss: 4.4728\n",
      "Iteration: 48 of 77\ttrain_loss: 4.4043\n",
      "Iteration: 50 of 77\ttrain_loss: 4.1697\n",
      "Iteration: 52 of 77\ttrain_loss: 4.0305\n",
      "Iteration: 54 of 77\ttrain_loss: 4.5983\n",
      "Iteration: 56 of 77\ttrain_loss: 4.2057\n",
      "Iteration: 58 of 77\ttrain_loss: 4.3718\n",
      "Iteration: 60 of 77\ttrain_loss: 4.5218\n",
      "Iteration: 62 of 77\ttrain_loss: 4.6080\n",
      "Iteration: 64 of 77\ttrain_loss: 4.5705\n",
      "Iteration: 66 of 77\ttrain_loss: 4.1878\n",
      "Iteration: 68 of 77\ttrain_loss: 4.1847\n",
      "Iteration: 70 of 77\ttrain_loss: 3.9928\n",
      "Iteration: 72 of 77\ttrain_loss: 4.0488\n",
      "Iteration: 74 of 77\ttrain_loss: 4.1822\n",
      "Iteration: 76 of 77\ttrain_loss: 3.9422\n",
      "Iteration: 77 of 77\ttrain_loss: 4.0338\n",
      "Average Score for this Epoch: 5.341090202331543\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 1 of 5 --------------------\n",
      "Iteration: 0 of 77\ttrain_loss: 3.9604\n",
      "Iteration: 2 of 77\ttrain_loss: 3.9426\n",
      "Iteration: 4 of 77\ttrain_loss: 3.9331\n",
      "Iteration: 6 of 77\ttrain_loss: 3.8434\n",
      "Iteration: 8 of 77\ttrain_loss: 3.6743\n",
      "Iteration: 10 of 77\ttrain_loss: 4.1766\n",
      "Iteration: 12 of 77\ttrain_loss: 3.4952\n",
      "Iteration: 14 of 77\ttrain_loss: 3.7560\n",
      "Iteration: 16 of 77\ttrain_loss: 3.7964\n",
      "Iteration: 18 of 77\ttrain_loss: 4.0462\n",
      "Iteration: 20 of 77\ttrain_loss: 4.0093\n",
      "Iteration: 22 of 77\ttrain_loss: 3.2554\n",
      "Iteration: 24 of 77\ttrain_loss: 3.3381\n",
      "Iteration: 26 of 77\ttrain_loss: 3.4522\n",
      "Iteration: 28 of 77\ttrain_loss: 3.6775\n",
      "Iteration: 30 of 77\ttrain_loss: 3.7576\n",
      "Iteration: 32 of 77\ttrain_loss: 3.2790\n",
      "Iteration: 34 of 77\ttrain_loss: 3.5426\n",
      "Iteration: 36 of 77\ttrain_loss: 3.5177\n",
      "Iteration: 38 of 77\ttrain_loss: 3.4325\n",
      "Iteration: 40 of 77\ttrain_loss: 3.3449\n",
      "Iteration: 42 of 77\ttrain_loss: 3.7406\n",
      "Iteration: 44 of 77\ttrain_loss: 3.2336\n",
      "Iteration: 46 of 77\ttrain_loss: 3.7773\n",
      "Iteration: 48 of 77\ttrain_loss: 3.2432\n",
      "Iteration: 50 of 77\ttrain_loss: 3.3225\n",
      "Iteration: 52 of 77\ttrain_loss: 3.4307\n",
      "Iteration: 54 of 77\ttrain_loss: 3.4252\n",
      "Iteration: 56 of 77\ttrain_loss: 3.2308\n",
      "Iteration: 58 of 77\ttrain_loss: 3.0829\n",
      "Iteration: 60 of 77\ttrain_loss: 2.9581\n",
      "Iteration: 62 of 77\ttrain_loss: 3.3591\n",
      "Iteration: 64 of 77\ttrain_loss: 3.5796\n",
      "Iteration: 66 of 77\ttrain_loss: 3.2633\n",
      "Iteration: 68 of 77\ttrain_loss: 3.1819\n",
      "Iteration: 70 of 77\ttrain_loss: 3.1776\n",
      "Iteration: 72 of 77\ttrain_loss: 3.3590\n",
      "Iteration: 74 of 77\ttrain_loss: 3.0122\n",
      "Iteration: 76 of 77\ttrain_loss: 3.3737\n",
      "Iteration: 77 of 77\ttrain_loss: 3.1332\n",
      "Average Score for this Epoch: 3.5306012630462646\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 2 of 5 --------------------\n",
      "Iteration: 0 of 77\ttrain_loss: 2.9480\n",
      "Iteration: 2 of 77\ttrain_loss: 3.0702\n",
      "Iteration: 4 of 77\ttrain_loss: 3.1858\n",
      "Iteration: 6 of 77\ttrain_loss: 3.1570\n",
      "Iteration: 8 of 77\ttrain_loss: 3.1597\n",
      "Iteration: 10 of 77\ttrain_loss: 3.1889\n",
      "Iteration: 12 of 77\ttrain_loss: 3.2031\n",
      "Iteration: 14 of 77\ttrain_loss: 2.6675\n",
      "Iteration: 16 of 77\ttrain_loss: 2.7705\n",
      "Iteration: 18 of 77\ttrain_loss: 3.2213\n",
      "Iteration: 20 of 77\ttrain_loss: 2.6156\n",
      "Iteration: 22 of 77\ttrain_loss: 2.7757\n",
      "Iteration: 24 of 77\ttrain_loss: 2.8349\n",
      "Iteration: 26 of 77\ttrain_loss: 3.4089\n",
      "Iteration: 28 of 77\ttrain_loss: 3.2092\n",
      "Iteration: 30 of 77\ttrain_loss: 2.6305\n",
      "Iteration: 32 of 77\ttrain_loss: 2.5034\n",
      "Iteration: 34 of 77\ttrain_loss: 2.8585\n",
      "Iteration: 36 of 77\ttrain_loss: 2.8099\n",
      "Iteration: 38 of 77\ttrain_loss: 3.0351\n",
      "Iteration: 40 of 77\ttrain_loss: 3.0004\n",
      "Iteration: 42 of 77\ttrain_loss: 3.0640\n",
      "Iteration: 44 of 77\ttrain_loss: 2.8298\n",
      "Iteration: 46 of 77\ttrain_loss: 2.8726\n",
      "Iteration: 48 of 77\ttrain_loss: 2.7157\n",
      "Iteration: 50 of 77\ttrain_loss: 2.7101\n",
      "Iteration: 52 of 77\ttrain_loss: 3.1216\n",
      "Iteration: 54 of 77\ttrain_loss: 2.6637\n",
      "Iteration: 56 of 77\ttrain_loss: 2.7907\n",
      "Iteration: 58 of 77\ttrain_loss: 2.8283\n",
      "Iteration: 60 of 77\ttrain_loss: 2.6878\n",
      "Iteration: 62 of 77\ttrain_loss: 2.6412\n",
      "Iteration: 64 of 77\ttrain_loss: 2.9095\n",
      "Iteration: 66 of 77\ttrain_loss: 2.6203\n",
      "Iteration: 68 of 77\ttrain_loss: 2.6347\n",
      "Iteration: 70 of 77\ttrain_loss: 2.6412\n",
      "Iteration: 72 of 77\ttrain_loss: 2.8242\n",
      "Iteration: 74 of 77\ttrain_loss: 2.6689\n",
      "Iteration: 76 of 77\ttrain_loss: 2.9497\n",
      "Iteration: 77 of 77\ttrain_loss: 2.6225\n",
      "Average Score for this Epoch: 2.880127191543579\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 3 of 5 --------------------\n",
      "Iteration: 0 of 77\ttrain_loss: 2.7115\n",
      "Iteration: 2 of 77\ttrain_loss: 2.4544\n",
      "Iteration: 4 of 77\ttrain_loss: 2.5307\n",
      "Iteration: 6 of 77\ttrain_loss: 2.5822\n",
      "Iteration: 8 of 77\ttrain_loss: 2.5785\n",
      "Iteration: 10 of 77\ttrain_loss: 2.4761\n",
      "Iteration: 12 of 77\ttrain_loss: 2.5875\n",
      "Iteration: 14 of 77\ttrain_loss: 2.5083\n",
      "Iteration: 16 of 77\ttrain_loss: 2.4561\n",
      "Iteration: 18 of 77\ttrain_loss: 2.6208\n",
      "Iteration: 20 of 77\ttrain_loss: 2.4428\n",
      "Iteration: 22 of 77\ttrain_loss: 2.7963\n",
      "Iteration: 24 of 77\ttrain_loss: 2.6052\n",
      "Iteration: 26 of 77\ttrain_loss: 2.2213\n",
      "Iteration: 28 of 77\ttrain_loss: 2.6189\n",
      "Iteration: 30 of 77\ttrain_loss: 2.4927\n",
      "Iteration: 32 of 77\ttrain_loss: 2.1797\n",
      "Iteration: 34 of 77\ttrain_loss: 2.4679\n",
      "Iteration: 36 of 77\ttrain_loss: 2.4787\n",
      "Iteration: 38 of 77\ttrain_loss: 2.2578\n",
      "Iteration: 40 of 77\ttrain_loss: 2.3809\n",
      "Iteration: 42 of 77\ttrain_loss: 2.5215\n",
      "Iteration: 44 of 77\ttrain_loss: 2.4165\n",
      "Iteration: 46 of 77\ttrain_loss: 2.5098\n",
      "Iteration: 48 of 77\ttrain_loss: 2.3730\n",
      "Iteration: 50 of 77\ttrain_loss: 2.4738\n",
      "Iteration: 52 of 77\ttrain_loss: 2.1142\n",
      "Iteration: 54 of 77\ttrain_loss: 2.3801\n",
      "Iteration: 56 of 77\ttrain_loss: 2.6243\n",
      "Iteration: 58 of 77\ttrain_loss: 2.5387\n",
      "Iteration: 60 of 77\ttrain_loss: 2.0508\n",
      "Iteration: 62 of 77\ttrain_loss: 2.3772\n",
      "Iteration: 64 of 77\ttrain_loss: 2.3629\n",
      "Iteration: 66 of 77\ttrain_loss: 2.3840\n",
      "Iteration: 68 of 77\ttrain_loss: 2.4865\n",
      "Iteration: 70 of 77\ttrain_loss: 2.5805\n",
      "Iteration: 72 of 77\ttrain_loss: 2.4486\n",
      "Iteration: 74 of 77\ttrain_loss: 2.6272\n",
      "Iteration: 76 of 77\ttrain_loss: 2.4497\n",
      "Iteration: 77 of 77\ttrain_loss: 2.4824\n",
      "Average Score for this Epoch: 2.474745035171509\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 4 of 5 --------------------\n",
      "Iteration: 0 of 77\ttrain_loss: 1.9411\n",
      "Iteration: 2 of 77\ttrain_loss: 2.2617\n",
      "Iteration: 4 of 77\ttrain_loss: 2.1502\n",
      "Iteration: 6 of 77\ttrain_loss: 2.2682\n",
      "Iteration: 8 of 77\ttrain_loss: 2.2683\n",
      "Iteration: 10 of 77\ttrain_loss: 2.0933\n",
      "Iteration: 12 of 77\ttrain_loss: 2.1410\n",
      "Iteration: 14 of 77\ttrain_loss: 1.8592\n",
      "Iteration: 16 of 77\ttrain_loss: 2.2955\n",
      "Iteration: 18 of 77\ttrain_loss: 2.0888\n",
      "Iteration: 20 of 77\ttrain_loss: 2.2485\n",
      "Iteration: 22 of 77\ttrain_loss: 2.1192\n",
      "Iteration: 24 of 77\ttrain_loss: 2.0462\n",
      "Iteration: 26 of 77\ttrain_loss: 2.1250\n",
      "Iteration: 28 of 77\ttrain_loss: 2.2431\n",
      "Iteration: 30 of 77\ttrain_loss: 1.9567\n",
      "Iteration: 32 of 77\ttrain_loss: 2.1837\n",
      "Iteration: 34 of 77\ttrain_loss: 2.2827\n",
      "Iteration: 36 of 77\ttrain_loss: 2.0907\n",
      "Iteration: 38 of 77\ttrain_loss: 2.1297\n",
      "Iteration: 40 of 77\ttrain_loss: 1.9473\n",
      "Iteration: 42 of 77\ttrain_loss: 2.2126\n",
      "Iteration: 44 of 77\ttrain_loss: 2.1823\n",
      "Iteration: 46 of 77\ttrain_loss: 2.2764\n",
      "Iteration: 48 of 77\ttrain_loss: 2.2523\n",
      "Iteration: 50 of 77\ttrain_loss: 2.3644\n",
      "Iteration: 52 of 77\ttrain_loss: 2.1742\n",
      "Iteration: 54 of 77\ttrain_loss: 2.0890\n",
      "Iteration: 56 of 77\ttrain_loss: 2.2330\n",
      "Iteration: 58 of 77\ttrain_loss: 2.2242\n",
      "Iteration: 60 of 77\ttrain_loss: 1.8600\n",
      "Iteration: 62 of 77\ttrain_loss: 2.1458\n",
      "Iteration: 64 of 77\ttrain_loss: 2.1683\n",
      "Iteration: 66 of 77\ttrain_loss: 2.0528\n",
      "Iteration: 68 of 77\ttrain_loss: 2.2818\n",
      "Iteration: 70 of 77\ttrain_loss: 2.1177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 72 of 77\ttrain_loss: 2.1395\n",
      "Iteration: 74 of 77\ttrain_loss: 2.1505\n",
      "Iteration: 76 of 77\ttrain_loss: 2.3596\n",
      "Iteration: 77 of 77\ttrain_loss: 2.1795\n",
      "Average Score for this Epoch: 2.1676385402679443\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 5 of 5 --------------------\n",
      "Iteration: 0 of 77\ttrain_loss: 2.2201\n",
      "Iteration: 2 of 77\ttrain_loss: 2.0553\n",
      "Iteration: 4 of 77\ttrain_loss: 1.6248\n",
      "Iteration: 6 of 77\ttrain_loss: 1.7976\n",
      "Iteration: 8 of 77\ttrain_loss: 1.6297\n",
      "Iteration: 10 of 77\ttrain_loss: 1.8938\n",
      "Iteration: 12 of 77\ttrain_loss: 1.9537\n",
      "Iteration: 14 of 77\ttrain_loss: 1.9417\n",
      "Iteration: 16 of 77\ttrain_loss: 1.9504\n",
      "Iteration: 18 of 77\ttrain_loss: 1.8164\n",
      "Iteration: 20 of 77\ttrain_loss: 2.0102\n",
      "Iteration: 22 of 77\ttrain_loss: 1.8986\n",
      "Iteration: 24 of 77\ttrain_loss: 1.9646\n",
      "Iteration: 26 of 77\ttrain_loss: 1.9346\n",
      "Iteration: 28 of 77\ttrain_loss: 2.1057\n",
      "Iteration: 30 of 77\ttrain_loss: 2.0597\n",
      "Iteration: 32 of 77\ttrain_loss: 2.0495\n",
      "Iteration: 34 of 77\ttrain_loss: 2.2313\n",
      "Iteration: 36 of 77\ttrain_loss: 2.0346\n",
      "Iteration: 38 of 77\ttrain_loss: 1.8536\n",
      "Iteration: 40 of 77\ttrain_loss: 2.1641\n",
      "Iteration: 42 of 77\ttrain_loss: 2.1095\n",
      "Iteration: 44 of 77\ttrain_loss: 2.1618\n",
      "Iteration: 46 of 77\ttrain_loss: 1.7455\n",
      "Iteration: 48 of 77\ttrain_loss: 1.8280\n",
      "Iteration: 50 of 77\ttrain_loss: 2.0368\n",
      "Iteration: 52 of 77\ttrain_loss: 1.8901\n",
      "Iteration: 54 of 77\ttrain_loss: 1.8177\n",
      "Iteration: 56 of 77\ttrain_loss: 2.0534\n",
      "Iteration: 58 of 77\ttrain_loss: 1.8921\n",
      "Iteration: 60 of 77\ttrain_loss: 1.8626\n",
      "Iteration: 62 of 77\ttrain_loss: 2.0220\n",
      "Iteration: 64 of 77\ttrain_loss: 1.8182\n",
      "Iteration: 66 of 77\ttrain_loss: 1.9089\n",
      "Iteration: 68 of 77\ttrain_loss: 1.7247\n",
      "Iteration: 70 of 77\ttrain_loss: 1.9399\n",
      "Iteration: 72 of 77\ttrain_loss: 1.9931\n",
      "Iteration: 74 of 77\ttrain_loss: 1.8763\n",
      "Iteration: 76 of 77\ttrain_loss: 1.8815\n",
      "Iteration: 77 of 77\ttrain_loss: 1.8445\n",
      "Average Score for this Epoch: 1.9258334636688232\n",
      "--- new best score ---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create the graph and train the model. \n",
    "nmt_model_utils.reset_graph()\n",
    "\n",
    "nmt = NMT_Model.NMT(en_word2ind,\n",
    "                    en_ind2word,\n",
    "                    de_word2ind,\n",
    "                    de_ind2word,\n",
    "                    './models/local_one/my_model',\n",
    "                    'TRAIN',\n",
    "                    embedding_dim = embedding_dim,\n",
    "                    num_layers_encoder = num_layers_encoder,\n",
    "                    num_layers_decoder = num_layers_decoder,\n",
    "                    batch_size = batch_size,\n",
    "                    clip = clip,\n",
    "                    keep_probability = keep_probability,\n",
    "                    learning_rate = learning_rate,\n",
    "                    epochs = epochs,\n",
    "                    rnn_size_encoder = rnn_size_encoder,\n",
    "                    rnn_size_decoder = rnn_size_decoder, \n",
    "                    learning_rate_decay_steps = learning_rate_decay_steps,\n",
    "                    learning_rate_decay = learning_rate_decay)\n",
    "  \n",
    "nmt.build_graph()\n",
    "nmt.train(en_inds, de_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_de_inds, _de_unknowns = nmt_data_utils.convert_to_inds(de_preprocessed_clean, de_word2ind, sos = True,  eos = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built.\n",
      "Restore graph from  ./models/local_one/my_model\n",
      "INFO:tensorflow:Restoring parameters from ./models/local_one/my_model\n"
     ]
    }
   ],
   "source": [
    "# the inference model does not necessaryly need to get input batches. we can just give it. the whole input\n",
    "# data, but the the batchsize has to be specified as the lenght of the input data.\n",
    "nmt_model_utils.reset_graph()\n",
    "\n",
    "nmt = NMT_Model.NMT(en_word2ind,\n",
    "                    en_ind2word,\n",
    "                    de_word2ind,\n",
    "                    de_ind2word,\n",
    "                    './models/local_one/my_model',\n",
    "                    'INFER',\n",
    "                    num_layers_encoder = num_layers_encoder,\n",
    "                    num_layers_decoder = num_layers_decoder,\n",
    "                    batch_size = len(en_inds[:50]),\n",
    "                    keep_probability = 1.0,\n",
    "                    learning_rate = 0.0,\n",
    "                    beam_width = 0,\n",
    "                    rnn_size_encoder = rnn_size_encoder,\n",
    "                    rnn_size_decoder = rnn_size_decoder)\n",
    "\n",
    "nmt.build_graph()\n",
    "preds = nmt.infer(en_inds[:50], restore_path =  './models/local_one/my_model', targets = _de_inds[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "resumption of the session\n",
      "\n",
      "Actual translation:\n",
      "wiederaufnahme der sitzungsperiode\n",
      "\n",
      "Created translation:\n",
      "wiederaufnahme frauen sitzungsperiode sitzungsperiode dialog\n",
      "\n",
      "Bleu-score: 1.4488496539373276e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "please rise , then , for this minute ' s silence .\n",
      "\n",
      "Actual translation:\n",
      "ich bitte sie , sich zu einer schweigeminute zu erheben .\n",
      "\n",
      "Created translation:\n",
      "einführung fordern erhebt erhebt erhebt erhebt aufgabe aufgabe aufgabe aufgabe aufgabe aufgabe bekommen\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "( the house rose and observed a minute ' s silence )\n",
      "\n",
      "Actual translation:\n",
      "( das parlament erhebt sich zu einer schweigeminute . )\n",
      "\n",
      "Created translation:\n",
      "( beifall erhebt erhebt erhebt erhebt erhebt erhebt erhebt lesung lesung lesung\n",
      "\n",
      "Bleu-score: 1.1640469867513693e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "madam president , on a point of order .\n",
      "\n",
      "Actual translation:\n",
      "frau präsidentin , zur geschäftsordnung .\n",
      "\n",
      "Created translation:\n",
      "soweit präsidentin stellungnahme stellungnahme hingewiesen hingewiesen hingewiesen hingewiesen\n",
      "\n",
      "Bleu-score: 1.0832677820940877e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "madam president , on a point of order .\n",
      "\n",
      "Actual translation:\n",
      "frau präsidentin , zur geschäftsordnung .\n",
      "\n",
      "Created translation:\n",
      "soweit präsidentin stellungnahme stellungnahme hingewiesen hingewiesen hingewiesen hingewiesen\n",
      "\n",
      "Bleu-score: 1.0832677820940877e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "thank you , mr segni , i shall do so gladly .\n",
      "\n",
      "Actual translation:\n",
      "vielen dank , herr segni , das will ich gerne tun .\n",
      "\n",
      "Created translation:\n",
      "mein dank ihnen ihnen ihnen ihnen ihnen ihnen ihnen ihnen ihnen gesagt gesagt\n",
      "\n",
      "Bleu-score: 9.594503055152632e-232\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "it is the case of alexander nikitin .\n",
      "\n",
      "Actual translation:\n",
      "das ist der fall von alexander nikitin .\n",
      "\n",
      "Created translation:\n",
      "hinzu läßt liegt grund grund grund wert wert wert wert\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "it will , i hope , be examined in a positive light .\n",
      "\n",
      "Actual translation:\n",
      "ich hoffe , daß dort in ihrem sinne entschieden wird .\n",
      "\n",
      "Created translation:\n",
      "wann werde werde werde werde gesagt gesagt gesagt gesagt gesagt gesagt gesagt gesagt\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "why are there no fire instructions ?\n",
      "\n",
      "Actual translation:\n",
      "warum finden keine brandschutzbelehrungen statt ?\n",
      "\n",
      "Created translation:\n",
      "warum könnte man man einmal einmal einmal tun\n",
      "\n",
      "Bleu-score: 1.0832677820940877e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "why are no-smoking areas not enforced ?\n",
      "\n",
      "Actual translation:\n",
      "warum wird in den nichtraucherzonen das rauchverbot nicht durchgesetzt ?\n",
      "\n",
      "Created translation:\n",
      "warum erwähne zeigt unternehmen dann union union union union union union union\n",
      "\n",
      "Bleu-score: 9.788429383461836e-232\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "mr berenguer fuster , we shall check all this .\n",
      "\n",
      "Actual translation:\n",
      "lieber kollege , wir werden das prüfen .\n",
      "\n",
      "Created translation:\n",
      "lieber abstimmung findet da ihnen alle alle alle alle diesem\n",
      "\n",
      "Bleu-score: 1.0244914152188952e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "we do not know what is happening .\n",
      "\n",
      "Actual translation:\n",
      "wir wissen nicht , was passiert .\n",
      "\n",
      "Created translation:\n",
      "kurzum dürfen dürfen dürfen jetzt tun tun tun tun\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "relating to wednesday :\n",
      "\n",
      "Actual translation:\n",
      "zum mittwoch :\n",
      "\n",
      "Created translation:\n",
      "gemeinsamer mittwoch mittwoch mittwoch mittwoch\n",
      "\n",
      "Bleu-score: 1.2183324802375697e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "( applause from the pse group )\n",
      "\n",
      "Actual translation:\n",
      "( beifall der pse-fraktion )\n",
      "\n",
      "Created translation:\n",
      "( beifall beifall folgt pse-fraktion links ;\n",
      "\n",
      "Bleu-score: 7.711523862191631e-155\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "mr hänsch represented you on this occasion .\n",
      "\n",
      "Actual translation:\n",
      "der kollege hänsch hat sie dort vertreten .\n",
      "\n",
      "Created translation:\n",
      "lieber sprach sprach sprach sprach sprach sprach sprach schluß schluß\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "we then put it to a vote .\n",
      "\n",
      "Actual translation:\n",
      "wir haben dann abgestimmt .\n",
      "\n",
      "Created translation:\n",
      "deshalb würden nochmals sprechen sprechen vor vor\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "there was a vote on this matter .\n",
      "\n",
      "Actual translation:\n",
      "es gab eine abstimmung zu diesem punkt .\n",
      "\n",
      "Created translation:\n",
      "zweitens gab findet findet da nun einmal einmal einmal einmal\n",
      "\n",
      "Bleu-score: 1.0244914152188952e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "all of the others were of a different opinion .\n",
      "\n",
      "Actual translation:\n",
      "alle anderen waren anderer meinung .\n",
      "\n",
      "Created translation:\n",
      "sodann sagt sagt sagt waren waren waren registriert\n",
      "\n",
      "Bleu-score: 1.0832677820940877e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "that was the decision .\n",
      "\n",
      "Actual translation:\n",
      "das war der beschluß .\n",
      "\n",
      "Created translation:\n",
      "darum widerspricht aspekt aspekt braucht braucht braucht\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "i should now like to comment on the issue itself .\n",
      "\n",
      "Actual translation:\n",
      "jetzt möchte ich zur sache selbst etwas sagen .\n",
      "\n",
      "Created translation:\n",
      "vielleicht meinung meinung meinung meinung meinung meinung meinung meinung meinung meinung\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "there is no such document !\n",
      "\n",
      "Actual translation:\n",
      "ein solches dokument gibt es nicht !\n",
      "\n",
      "Created translation:\n",
      "deswegen handelt endlich endlich meinung fortschritte sein sein !\n",
      "\n",
      "Bleu-score: 1.0518351895246305e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "we have agreed to this .\n",
      "\n",
      "Actual translation:\n",
      "wir haben dem zugestimmt .\n",
      "\n",
      "Created translation:\n",
      "deshalb sollten alle alle schon einmal einmal\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "( applause from the ppe-de group )\n",
      "\n",
      "Actual translation:\n",
      "( beifall von der ppe-de-fraktion )\n",
      "\n",
      "Created translation:\n",
      "( beifall beifall beifall ( ( ( (\n",
      "\n",
      "Bleu-score: 6.484592771860512e-155\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "( parliament rejected the request ) president .\n",
      "\n",
      "Actual translation:\n",
      "( das parlament lehnt den antrag ab . ) die präsidentin .\n",
      "\n",
      "Created translation:\n",
      "( parlament lehnt lehnt lehnt lehnt lehnt lehnt ( antrag ab ab ab\n",
      "\n",
      "Bleu-score: 7.505697654413981e-155\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "thank you , mr poettering .\n",
      "\n",
      "Actual translation:\n",
      "vielen dank , herr poettering .\n",
      "\n",
      "Created translation:\n",
      "bewerten dank ihm ihnen ihnen ihnen ihnen ihnen\n",
      "\n",
      "Bleu-score: 1.0832677820940877e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "it is not a lot to ask .\n",
      "\n",
      "Actual translation:\n",
      "das ist nicht zuviel verlangt .\n",
      "\n",
      "Created translation:\n",
      "hinzu läßt läßt läßt läßt läßt meinung gut\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "madam president , has my vote been counted ?\n",
      "\n",
      "Actual translation:\n",
      "frau präsidentin ! ist meine stimme mitgezählt worden ?\n",
      "\n",
      "Created translation:\n",
      "wenn präsidentin präsident dagegen dagegen dagegen dagegen dagegen dagegen dazu so\n",
      "\n",
      "Bleu-score: 1.0003688322288243e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "my vote was `` in favour '' .\n",
      "\n",
      "Actual translation:\n",
      "ich habe `` dafür `` gestimmt .\n",
      "\n",
      "Created translation:\n",
      "mein präsident stellungnahme stellungnahme meinung punkt . .\n",
      "\n",
      "Bleu-score: 1.0832677820940877e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "there is no room for amendments .\n",
      "\n",
      "Actual translation:\n",
      "änderungen sind nicht möglich .\n",
      "\n",
      "Created translation:\n",
      "da sollte liegt aber einmal einmal einmal\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "this is an important matter .\n",
      "\n",
      "Actual translation:\n",
      "das ist eine wichtige angelegenheit .\n",
      "\n",
      "Created translation:\n",
      "darum handelt grund grund grund wichtig wichtig wichtig\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "you did not call me either .\n",
      "\n",
      "Actual translation:\n",
      "sie haben mich auch nicht aufgerufen .\n",
      "\n",
      "Created translation:\n",
      "mein hatte endlich endlich endlich endlich endlich genetisch organismen\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "i am terribly sorry , mr hänsch and mr cox .\n",
      "\n",
      "Actual translation:\n",
      "das tut mir leid , herr hänsch und herr cox .\n",
      "\n",
      "Created translation:\n",
      "meine werde ihnen ihnen ihnen einmal einmal einmal einmal einmal einmal einmal einmal\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "i did not see you asking to speak .\n",
      "\n",
      "Actual translation:\n",
      "ich hatte nicht gesehen , daß sie ums wort gebeten hatten .\n",
      "\n",
      "Created translation:\n",
      "mein fordere meinung meinung meinung meinung meinung meinung meinung bitten machen machen machen\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "this seems to me to be a workable solution .\n",
      "\n",
      "Actual translation:\n",
      "ich halte dieses vorgehen für angemessen .\n",
      "\n",
      "Created translation:\n",
      "all halte halte meines erachtens erachtens etwas etwas bereich\n",
      "\n",
      "Bleu-score: 1.0518351895246305e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "( the order of business was adopted thus amended )\n",
      "\n",
      "Actual translation:\n",
      "( das parlament genehmigt den geänderten arbeitsplan . )\n",
      "\n",
      "Created translation:\n",
      "( das die der der der der der der der der\n",
      "\n",
      "Bleu-score: 5.477489369001354e-155\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "i would urge you to endorse this .\n",
      "\n",
      "Actual translation:\n",
      "ich bitte sie um zustimmung .\n",
      "\n",
      "Created translation:\n",
      "vielleicht werde gerne gerne nun tun tun tun\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "i congratulate him on his excellent report .\n",
      "\n",
      "Actual translation:\n",
      "ich beglückwünsche ihn zu seinem ausgezeichneten bericht .\n",
      "\n",
      "Created translation:\n",
      "ich ich ich den den den den den den den\n",
      "\n",
      "Bleu-score: 1.0244914152188952e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "this is a pity , in a sense .\n",
      "\n",
      "Actual translation:\n",
      "in gewissem sinne bedauere ich das .\n",
      "\n",
      "Created translation:\n",
      "deshalb halte endlich endlich endlich endlich meinung punkt land\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "in short , the issue is an important one .\n",
      "\n",
      "Actual translation:\n",
      "kurzum , ein ernstes problem .\n",
      "\n",
      "Created translation:\n",
      "kurzum handelt endlich endlich wichtiger wichtiger meinung punkt\n",
      "\n",
      "Bleu-score: 1.0832677820940877e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "i would like to mention one final point .\n",
      "\n",
      "Actual translation:\n",
      "gestatten sie mir noch einen letzten hinweis .\n",
      "\n",
      "Created translation:\n",
      "meiner werde meiner meiner meiner meiner erachtens einmal einmal einmal\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "this , however , does not seem feasible .\n",
      "\n",
      "Actual translation:\n",
      "allerdings scheint das nicht machbar zu sein .\n",
      "\n",
      "Created translation:\n",
      "natürlich gilt aber aber einmal einmal einmal einmal einmal einmal\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "i should like to make just a few comments .\n",
      "\n",
      "Actual translation:\n",
      "ich möchte nur wenige anmerkungen machen .\n",
      "\n",
      "Created translation:\n",
      "vielleicht werde meines meines erachtens erachtens erachtens erachtens einmal\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "my third point has also been mentioned already .\n",
      "\n",
      "Actual translation:\n",
      "der dritte punkt wurde auch schon erwähnt .\n",
      "\n",
      "Created translation:\n",
      "herr gilt gilt gilt gilt gilt gilt gilt auch auch\n",
      "\n",
      "Bleu-score: 1.0244914152188952e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "the debate is closed .\n",
      "\n",
      "Actual translation:\n",
      "die aussprache ist geschlossen .\n",
      "\n",
      "Created translation:\n",
      "deshalb aussprache aussprache hiermit hiermit aussprache aussprache\n",
      "\n",
      "Bleu-score: 1.1200407237786664e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "the vote will take place tomorrow at 12 p.m .\n",
      "\n",
      "Actual translation:\n",
      "die abstimmung findet morgen um 12.00 uhr statt .\n",
      "\n",
      "Created translation:\n",
      "abstimmung abstimmung findet morgen 12.00 12.00 12.00 12.00 12.00 12.00 12.00\n",
      "\n",
      "Bleu-score: 3.661843723291765e-78\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "transport of dangerous goods by road\n",
      "\n",
      "Actual translation:\n",
      "gefahrguttransport auf der straße\n",
      "\n",
      "Created translation:\n",
      "indigene lage lage gericht gericht gericht\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "i thought that it was quite superb .\n",
      "\n",
      "Actual translation:\n",
      "ich fand das ganz hervorragend .\n",
      "\n",
      "Created translation:\n",
      "meiner halte meiner meiner meiner meiner erachtens erachtens\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "this directive is a contribution to this .\n",
      "\n",
      "Actual translation:\n",
      "diese richtlinie ist ein beitrag dazu .\n",
      "\n",
      "Created translation:\n",
      "darum handelt läßt läßt läßt läßt meinung punkt jahren\n",
      "\n",
      "Bleu-score: 0\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "the debate is closed .\n",
      "\n",
      "Actual translation:\n",
      "die aussprache ist geschlossen .\n",
      "\n",
      "Created translation:\n",
      "deshalb aussprache aussprache hiermit hiermit aussprache aussprache\n",
      "\n",
      "Bleu-score: 1.1200407237786664e-231\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "the vote will take place tomorrow at 12 p.m .\n",
      "\n",
      "Actual translation:\n",
      "die abstimmung findet morgen um 12.00 uhr statt .\n",
      "\n",
      "Created translation:\n",
      "abstimmung abstimmung findet morgen 12.00 12.00 12.00 12.00 12.00 12.00 12.00\n",
      "\n",
      "Bleu-score: 3.661843723291765e-78\n",
      "\n",
      "\n",
      "\n",
      "Total Bleu Score: 1.464737489316706e-79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pares\\Anaconda3\\envs\\tfp3.6\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\pares\\Anaconda3\\envs\\tfp3.6\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\pares\\Anaconda3\\envs\\tfp3.6\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# show some of the created translations\n",
    "# Note: the way bleu score is probably not the perfect way to do it\n",
    "nmt_model_utils.sample_results(preds, en_ind2word, de_ind2word, en_word2ind, de_word2ind, _de_inds[:50], en_inds[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
